{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Course 3 - Week 4 - Lesson 1 - Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sussahoo/Keras-ML/blob/master/NLP-WordGeneration-LSTM-.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOwsuGQQY9OL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRnDnCW-Z7qv",
        "outputId": "71d96897-f372-4a59-b04e-348558561a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "data=\"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\"\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1 # we want to generate OOV kind of token, total_words is used in last dense as well as for one hot encoding of labels\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'replied': 246, 'all': 5, 'forget': 87, 'just': 48, 'being': 254, 'able': 227, 'catchers': 146, 'soon': 53, 'cheeks': 208, 'through': 186, 'go': 200, 'colleen': 230, 'chaneys': 237, 'ground': 81, 'tore': 233, 'jig': 162, 'some': 212, 'nolans': 129, 'cried': 191, 'reel': 161, 'father': 74, 'young': 182, 'acres': 80, 'to': 13, 'under': 234, 'weeks': 30, 'might': 104, 'hall': 141, 'gave': 82, 'town': 66, 'them': 57, 'his': 16, 'around': 177, 'chanters': 259, 'big': 243, 'swore': 198, 'entangled': 261, 'they': 19, 'new': 37, 'ogradys': 131, 'him': 33, 'minute': 106, 'groups': 179, 'gray': 144, 'banished': 158, 'potatoes': 125, 'ned': 223, 'mcgilligan': 120, 'sweetheart': 222, 'fainted': 64, 'kerrigan': 207, 'steps': 38, 'she': 14, 'small': 217, 'athy': 67, 'creature': 190, 'round': 25, 'stretched': 231, 'squeezed': 256, 'declared': 214, 'learn': 174, 'old': 55, 'ten': 79, 'doing': 150, 'tea': 128, 'bacon': 127, 'sure': 101, 'further': 202, 'odaly': 111, 'drop': 218, 'out': 32, 'row': 206, 'jeremy': 69, 'for': 7, 'singing': 148, 'away': 40, 'piper': 252, 'near': 253, 'mccarthy': 184, 'then': 240, 'pipes': 257, 'got': 23, 'fall': 171, 'lashings': 121, 'youll': 90, 'red': 210, 'be': 100, 'we': 157, 'kicked': 248, 'leg': 62, 'youd': 167, 'gathered': 196, 'lanigans': 9, 'punch': 122, 'free': 102, 'water': 136, 'peggy': 119, 'brooks': 172, 'merry': 50, 'murther': 193, 'put': 61, 'strangled': 255, 'come': 88, 'went': 134, 'both': 107, 'accident': 180, 'daughter': 147, 'would': 170, 'nelly': 143, 'cakes': 126, 'of': 8, 'ill': 93, 'ach': 163, 'dublin': 59, 'terrible': 249, 'meelia': 192, 'invitation': 103, 'arrived': 117, 'had': 203, 'taras': 140, 'think': 168, 'courting': 132, 'ladies': 124, 'runctions': 241, 'danced': 58, 'wall': 45, 'sweet': 142, 'ructions': 99, 'powerful': 226, 'nonsense': 159, 'julia': 156, 'one': 68, 'satisfaction': 204, 'ask': 105, 'right': 185, 'relations': 43, 'learning': 36, 'twas': 239, 'table': 235, 'miss': 63, 'your': 95, 'once': 138, 'little': 112, 'rows': 98, 'from': 65, 'glisten': 97, 'her': 27, 'songs': 133, 'introduction': 247, 'hoops': 188, 'there': 28, 'girls': 17, 'three': 29, 'long': 39, 'by': 232, 'hed': 199, 'their': 56, 'bees': 108, 'call': 116, 'too': 219, 'ribbons': 260, 'bellows': 258, 'was': 34, 'listen': 92, 'eyes': 96, 'dolans': 130, 'that': 26, 'painted': 215, 'academy': 173, 'mchugh': 245, 'carmody': 197, 'took': 216, 'but': 91, 'casey': 251, 'boys': 24, 'hadnt': 72, 'midst': 205, 'with': 118, 'he': 21, 'me': 52, 'couples': 178, 'myself': 46, 'made': 76, 'room': 154, 'oh': 238, 'up': 31, 'cask': 109, 'judy': 110, 'terrance': 183, 'morgan': 224, 'were': 11, 'smashed': 236, 'happened': 181, 'didnt': 86, 'called': 194, 'battered': 71, 'and': 1, 'pound': 73, 'give': 115, 'in': 4, 'lads': 213, 'ceiling': 169, 'an': 60, 'twist': 160, 'as': 18, 'hullabaloo': 250, 'at': 12, 'lanigan': 70, 'saw': 228, 'whirligig': 155, 'if': 89, 'again': 22, 'end': 262, 'dancing': 49, 'make': 94, 'when': 44, 'same': 209, 'rat': 145, 'til': 20, 'how': 165, 'phelim': 244, 'rose': 211, 'grand': 83, 'party': 84, 'stepped': 15, 'harp': 137, 'lick': 242, 'nice': 47, 'poor': 189, 'ball': 10, 'kinds': 151, 'brothers': 195, 'farm': 78, 'suppose': 221, 'who': 85, 'polkas': 153, 'much': 220, 'tipped': 51, 'mad': 166, 'plenty': 135, 'nothing': 175, 'friends': 42, 'milliner': 113, 'died': 75, 'man': 77, 'a': 3, 'finnertys': 187, 'mavrone': 164, 'i': 6, 'no': 201, 'spent': 35, 'together': 149, 'wink': 114, 'hearty': 176, 'so': 225, 'time': 54, 'fair': 229, 'the': 2, 'wine': 123, 'nonsensical': 152, 'sounded': 139, 'left': 41}\n",
            "263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soPGVheskaQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJtwVB2NbOAP",
        "colab_type": "code",
        "outputId": "c96dfe72-46ac-40b4-e3c0-f243fbece9fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(tokenizer.word_index['in'])\n",
        "print(tokenizer.word_index['the'])\n",
        "print(tokenizer.word_index['town'])\n",
        "print(tokenizer.word_index['of'])\n",
        "print(tokenizer.word_index['athy'])\n",
        "print(tokenizer.word_index['one'])\n",
        "print(tokenizer.word_index['jeremy'])\n",
        "print(tokenizer.word_index['lanigan'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "2\n",
            "66\n",
            "8\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49Cv68JOakwv",
        "colab_type": "code",
        "outputId": "7b426149-2f05-496f-b6a2-f7b7748d8e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(xs[6])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  4  2 66  8 67 68 69]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY-jwvfgbEF8",
        "colab_type": "code",
        "outputId": "bc539168-3742-4618-a4f1-689f9dc64188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print(ys[6])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtzlUMYadhKt",
        "colab_type": "code",
        "outputId": "b1175578-d384-4311-ee08-a4c42b2ef5e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print(xs[5])\n",
        "print(ys[5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  4  2 66  8 67 68]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4myRpB1c4Gg",
        "colab_type": "code",
        "outputId": "ef2cb622-ba12-4372-f016-ed33426d33d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'replied': 246, 'all': 5, 'forget': 87, 'just': 48, 'being': 254, 'able': 227, 'catchers': 146, 'soon': 53, 'cheeks': 208, 'through': 186, 'go': 200, 'colleen': 230, 'chaneys': 237, 'ground': 81, 'tore': 233, 'jig': 162, 'some': 212, 'nolans': 129, 'cried': 191, 'reel': 161, 'father': 74, 'young': 182, 'acres': 80, 'to': 13, 'under': 234, 'weeks': 30, 'might': 104, 'hall': 141, 'gave': 82, 'town': 66, 'them': 57, 'his': 16, 'around': 177, 'chanters': 259, 'big': 243, 'swore': 198, 'entangled': 261, 'they': 19, 'new': 37, 'ogradys': 131, 'him': 33, 'minute': 106, 'groups': 179, 'gray': 144, 'banished': 158, 'potatoes': 125, 'ned': 223, 'mcgilligan': 120, 'sweetheart': 222, 'fainted': 64, 'kerrigan': 207, 'steps': 38, 'she': 14, 'small': 217, 'athy': 67, 'creature': 190, 'round': 25, 'stretched': 231, 'squeezed': 256, 'declared': 214, 'learn': 174, 'old': 55, 'ten': 79, 'doing': 150, 'tea': 128, 'bacon': 127, 'sure': 101, 'further': 202, 'odaly': 111, 'drop': 218, 'out': 32, 'row': 206, 'jeremy': 69, 'for': 7, 'singing': 148, 'away': 40, 'piper': 252, 'near': 253, 'mccarthy': 184, 'then': 240, 'pipes': 257, 'got': 23, 'fall': 171, 'lashings': 121, 'youll': 90, 'red': 210, 'be': 100, 'we': 157, 'kicked': 248, 'leg': 62, 'youd': 167, 'gathered': 196, 'lanigans': 9, 'punch': 122, 'free': 102, 'water': 136, 'peggy': 119, 'brooks': 172, 'merry': 50, 'murther': 193, 'put': 61, 'strangled': 255, 'come': 88, 'went': 134, 'both': 107, 'accident': 180, 'daughter': 147, 'would': 170, 'nelly': 143, 'cakes': 126, 'of': 8, 'ill': 93, 'ach': 163, 'dublin': 59, 'terrible': 249, 'meelia': 192, 'invitation': 103, 'arrived': 117, 'had': 203, 'taras': 140, 'think': 168, 'courting': 132, 'ladies': 124, 'runctions': 241, 'danced': 58, 'wall': 45, 'sweet': 142, 'ructions': 99, 'powerful': 226, 'nonsense': 159, 'julia': 156, 'one': 68, 'satisfaction': 204, 'ask': 105, 'right': 185, 'relations': 43, 'learning': 36, 'twas': 239, 'table': 235, 'miss': 63, 'your': 95, 'once': 138, 'little': 112, 'rows': 98, 'from': 65, 'glisten': 97, 'her': 27, 'songs': 133, 'introduction': 247, 'hoops': 188, 'there': 28, 'girls': 17, 'three': 29, 'long': 39, 'by': 232, 'hed': 199, 'their': 56, 'bees': 108, 'call': 116, 'too': 219, 'ribbons': 260, 'bellows': 258, 'was': 34, 'listen': 92, 'eyes': 96, 'dolans': 130, 'that': 26, 'painted': 215, 'academy': 173, 'mchugh': 245, 'carmody': 197, 'took': 216, 'but': 91, 'casey': 251, 'boys': 24, 'hadnt': 72, 'midst': 205, 'with': 118, 'he': 21, 'me': 52, 'couples': 178, 'myself': 46, 'made': 76, 'room': 154, 'oh': 238, 'up': 31, 'cask': 109, 'judy': 110, 'terrance': 183, 'morgan': 224, 'were': 11, 'smashed': 236, 'happened': 181, 'didnt': 86, 'called': 194, 'battered': 71, 'and': 1, 'pound': 73, 'give': 115, 'in': 4, 'lads': 213, 'ceiling': 169, 'an': 60, 'twist': 160, 'as': 18, 'hullabaloo': 250, 'at': 12, 'lanigan': 70, 'saw': 228, 'whirligig': 155, 'if': 89, 'again': 22, 'end': 262, 'dancing': 49, 'make': 94, 'when': 44, 'same': 209, 'rat': 145, 'til': 20, 'how': 165, 'phelim': 244, 'rose': 211, 'grand': 83, 'party': 84, 'stepped': 15, 'harp': 137, 'lick': 242, 'nice': 47, 'poor': 189, 'ball': 10, 'kinds': 151, 'brothers': 195, 'farm': 78, 'suppose': 221, 'who': 85, 'polkas': 153, 'much': 220, 'tipped': 51, 'mad': 166, 'plenty': 135, 'nothing': 175, 'friends': 42, 'milliner': 113, 'died': 75, 'man': 77, 'a': 3, 'finnertys': 187, 'mavrone': 164, 'i': 6, 'no': 201, 'spent': 35, 'together': 149, 'wink': 114, 'hearty': 176, 'so': 225, 'time': 54, 'fair': 229, 'the': 2, 'wine': 123, 'nonsensical': 152, 'sounded': 139, 'left': 41}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9vH8Y59ajYL",
        "colab_type": "code",
        "outputId": "0937ed97-7bcb-4a4c-c079-205ba550b1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "  model = Sequential()\n",
        "  model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "  model.add(Bidirectional(LSTM(20)))\n",
        "  model.add(Dense(total_words, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  history = model.fit(xs, ys, epochs=500, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 453 samples\n",
            "Epoch 1/500\n",
            "453/453 [==============================] - 8s 18ms/sample - loss: 5.5687 - accuracy: 0.0088\n",
            "Epoch 2/500\n",
            "453/453 [==============================] - 0s 328us/sample - loss: 5.5425 - accuracy: 0.0243\n",
            "Epoch 3/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 5.4735 - accuracy: 0.0265\n",
            "Epoch 4/500\n",
            "453/453 [==============================] - 0s 298us/sample - loss: 5.2909 - accuracy: 0.0353\n",
            "Epoch 5/500\n",
            "453/453 [==============================] - 0s 297us/sample - loss: 5.1271 - accuracy: 0.0508\n",
            "Epoch 6/500\n",
            "453/453 [==============================] - 0s 283us/sample - loss: 5.0671 - accuracy: 0.0508\n",
            "Epoch 7/500\n",
            "453/453 [==============================] - 0s 293us/sample - loss: 5.0273 - accuracy: 0.0464\n",
            "Epoch 8/500\n",
            "453/453 [==============================] - 0s 264us/sample - loss: 4.9942 - accuracy: 0.0530\n",
            "Epoch 9/500\n",
            "453/453 [==============================] - 0s 318us/sample - loss: 4.9625 - accuracy: 0.0574\n",
            "Epoch 10/500\n",
            "453/453 [==============================] - 0s 305us/sample - loss: 4.9259 - accuracy: 0.0530\n",
            "Epoch 11/500\n",
            "453/453 [==============================] - 0s 280us/sample - loss: 4.8868 - accuracy: 0.0574\n",
            "Epoch 12/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 4.8437 - accuracy: 0.0552\n",
            "Epoch 13/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 4.7958 - accuracy: 0.0552\n",
            "Epoch 14/500\n",
            "453/453 [==============================] - 0s 307us/sample - loss: 4.7427 - accuracy: 0.0530\n",
            "Epoch 15/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 4.6938 - accuracy: 0.0552\n",
            "Epoch 16/500\n",
            "453/453 [==============================] - 0s 322us/sample - loss: 4.6583 - accuracy: 0.0706\n",
            "Epoch 17/500\n",
            "453/453 [==============================] - 0s 299us/sample - loss: 4.6134 - accuracy: 0.0596\n",
            "Epoch 18/500\n",
            "453/453 [==============================] - 0s 324us/sample - loss: 4.5700 - accuracy: 0.0728\n",
            "Epoch 19/500\n",
            "453/453 [==============================] - 0s 296us/sample - loss: 4.5320 - accuracy: 0.0773\n",
            "Epoch 20/500\n",
            "453/453 [==============================] - 0s 287us/sample - loss: 4.4865 - accuracy: 0.0817\n",
            "Epoch 21/500\n",
            "453/453 [==============================] - 0s 266us/sample - loss: 4.4475 - accuracy: 0.0927\n",
            "Epoch 22/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 4.4086 - accuracy: 0.0905\n",
            "Epoch 23/500\n",
            "453/453 [==============================] - 0s 295us/sample - loss: 4.3747 - accuracy: 0.1082\n",
            "Epoch 24/500\n",
            "453/453 [==============================] - 0s 321us/sample - loss: 4.3365 - accuracy: 0.1148\n",
            "Epoch 25/500\n",
            "453/453 [==============================] - 0s 299us/sample - loss: 4.3062 - accuracy: 0.1192\n",
            "Epoch 26/500\n",
            "453/453 [==============================] - 0s 268us/sample - loss: 4.2672 - accuracy: 0.1325\n",
            "Epoch 27/500\n",
            "453/453 [==============================] - 0s 298us/sample - loss: 4.2342 - accuracy: 0.1280\n",
            "Epoch 28/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 4.1917 - accuracy: 0.1501\n",
            "Epoch 29/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 4.1637 - accuracy: 0.1435\n",
            "Epoch 30/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 4.1268 - accuracy: 0.1369\n",
            "Epoch 31/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 4.0985 - accuracy: 0.1545\n",
            "Epoch 32/500\n",
            "453/453 [==============================] - 0s 313us/sample - loss: 4.0573 - accuracy: 0.1413\n",
            "Epoch 33/500\n",
            "453/453 [==============================] - 0s 313us/sample - loss: 4.0246 - accuracy: 0.1567\n",
            "Epoch 34/500\n",
            "453/453 [==============================] - 0s 266us/sample - loss: 3.9819 - accuracy: 0.1567\n",
            "Epoch 35/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 3.9554 - accuracy: 0.1611\n",
            "Epoch 36/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 3.9111 - accuracy: 0.1744\n",
            "Epoch 37/500\n",
            "453/453 [==============================] - 0s 321us/sample - loss: 3.8690 - accuracy: 0.1611\n",
            "Epoch 38/500\n",
            "453/453 [==============================] - 0s 299us/sample - loss: 3.8387 - accuracy: 0.1678\n",
            "Epoch 39/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 3.8095 - accuracy: 0.1810\n",
            "Epoch 40/500\n",
            "453/453 [==============================] - 0s 309us/sample - loss: 3.7664 - accuracy: 0.1876\n",
            "Epoch 41/500\n",
            "453/453 [==============================] - 0s 304us/sample - loss: 3.7371 - accuracy: 0.1832\n",
            "Epoch 42/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 3.6948 - accuracy: 0.1921\n",
            "Epoch 43/500\n",
            "453/453 [==============================] - 0s 295us/sample - loss: 3.6682 - accuracy: 0.1987\n",
            "Epoch 44/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 3.6350 - accuracy: 0.2208\n",
            "Epoch 45/500\n",
            "453/453 [==============================] - 0s 305us/sample - loss: 3.5915 - accuracy: 0.2384\n",
            "Epoch 46/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 3.5584 - accuracy: 0.2406\n",
            "Epoch 47/500\n",
            "453/453 [==============================] - 0s 301us/sample - loss: 3.5229 - accuracy: 0.2450\n",
            "Epoch 48/500\n",
            "453/453 [==============================] - 0s 303us/sample - loss: 3.4913 - accuracy: 0.2605\n",
            "Epoch 49/500\n",
            "453/453 [==============================] - 0s 284us/sample - loss: 3.4594 - accuracy: 0.2605\n",
            "Epoch 50/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 3.4241 - accuracy: 0.2671\n",
            "Epoch 51/500\n",
            "453/453 [==============================] - 0s 295us/sample - loss: 3.3999 - accuracy: 0.2936\n",
            "Epoch 52/500\n",
            "453/453 [==============================] - 0s 297us/sample - loss: 3.3674 - accuracy: 0.3068\n",
            "Epoch 53/500\n",
            "453/453 [==============================] - 0s 329us/sample - loss: 3.3369 - accuracy: 0.3024\n",
            "Epoch 54/500\n",
            "453/453 [==============================] - 0s 316us/sample - loss: 3.3078 - accuracy: 0.3201\n",
            "Epoch 55/500\n",
            "453/453 [==============================] - 0s 306us/sample - loss: 3.2698 - accuracy: 0.3355\n",
            "Epoch 56/500\n",
            "453/453 [==============================] - 0s 299us/sample - loss: 3.2349 - accuracy: 0.3311\n",
            "Epoch 57/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 3.1984 - accuracy: 0.3377\n",
            "Epoch 58/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 3.1689 - accuracy: 0.3598\n",
            "Epoch 59/500\n",
            "453/453 [==============================] - 0s 276us/sample - loss: 3.1427 - accuracy: 0.3642\n",
            "Epoch 60/500\n",
            "453/453 [==============================] - 0s 301us/sample - loss: 3.1092 - accuracy: 0.3709\n",
            "Epoch 61/500\n",
            "453/453 [==============================] - 0s 287us/sample - loss: 3.0774 - accuracy: 0.3819\n",
            "Epoch 62/500\n",
            "453/453 [==============================] - 0s 333us/sample - loss: 3.0521 - accuracy: 0.3951\n",
            "Epoch 63/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 3.0344 - accuracy: 0.3863\n",
            "Epoch 64/500\n",
            "453/453 [==============================] - 0s 260us/sample - loss: 3.0007 - accuracy: 0.4106\n",
            "Epoch 65/500\n",
            "453/453 [==============================] - 0s 270us/sample - loss: 2.9768 - accuracy: 0.3974\n",
            "Epoch 66/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 2.9522 - accuracy: 0.4327\n",
            "Epoch 67/500\n",
            "453/453 [==============================] - 0s 316us/sample - loss: 2.9316 - accuracy: 0.4084\n",
            "Epoch 68/500\n",
            "453/453 [==============================] - 0s 297us/sample - loss: 2.8843 - accuracy: 0.4658\n",
            "Epoch 69/500\n",
            "453/453 [==============================] - 0s 333us/sample - loss: 2.8501 - accuracy: 0.4636\n",
            "Epoch 70/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 2.8333 - accuracy: 0.4702\n",
            "Epoch 71/500\n",
            "453/453 [==============================] - 0s 293us/sample - loss: 2.8026 - accuracy: 0.4879\n",
            "Epoch 72/500\n",
            "453/453 [==============================] - 0s 269us/sample - loss: 2.7743 - accuracy: 0.4834\n",
            "Epoch 73/500\n",
            "453/453 [==============================] - 0s 299us/sample - loss: 2.7456 - accuracy: 0.4879\n",
            "Epoch 74/500\n",
            "453/453 [==============================] - 0s 330us/sample - loss: 2.7167 - accuracy: 0.4989\n",
            "Epoch 75/500\n",
            "453/453 [==============================] - 0s 275us/sample - loss: 2.6826 - accuracy: 0.5099\n",
            "Epoch 76/500\n",
            "453/453 [==============================] - 0s 269us/sample - loss: 2.6509 - accuracy: 0.5166\n",
            "Epoch 77/500\n",
            "453/453 [==============================] - 0s 298us/sample - loss: 2.6341 - accuracy: 0.5077\n",
            "Epoch 78/500\n",
            "453/453 [==============================] - 0s 300us/sample - loss: 2.6032 - accuracy: 0.5188\n",
            "Epoch 79/500\n",
            "453/453 [==============================] - 0s 268us/sample - loss: 2.5758 - accuracy: 0.5254\n",
            "Epoch 80/500\n",
            "453/453 [==============================] - 0s 300us/sample - loss: 2.5518 - accuracy: 0.5232\n",
            "Epoch 81/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 2.5276 - accuracy: 0.5320\n",
            "Epoch 82/500\n",
            "453/453 [==============================] - 0s 299us/sample - loss: 2.5038 - accuracy: 0.5320\n",
            "Epoch 83/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 2.4822 - accuracy: 0.5342\n",
            "Epoch 84/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 2.4536 - accuracy: 0.5408\n",
            "Epoch 85/500\n",
            "453/453 [==============================] - 0s 303us/sample - loss: 2.4342 - accuracy: 0.5453\n",
            "Epoch 86/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 2.4233 - accuracy: 0.5497\n",
            "Epoch 87/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 2.3970 - accuracy: 0.5475\n",
            "Epoch 88/500\n",
            "453/453 [==============================] - 0s 277us/sample - loss: 2.3623 - accuracy: 0.5695\n",
            "Epoch 89/500\n",
            "453/453 [==============================] - 0s 295us/sample - loss: 2.3358 - accuracy: 0.5762\n",
            "Epoch 90/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 2.3104 - accuracy: 0.5784\n",
            "Epoch 91/500\n",
            "453/453 [==============================] - 0s 271us/sample - loss: 2.3035 - accuracy: 0.5960\n",
            "Epoch 92/500\n",
            "453/453 [==============================] - 0s 325us/sample - loss: 2.2713 - accuracy: 0.5916\n",
            "Epoch 93/500\n",
            "453/453 [==============================] - 0s 303us/sample - loss: 2.2476 - accuracy: 0.5828\n",
            "Epoch 94/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 2.2242 - accuracy: 0.6004\n",
            "Epoch 95/500\n",
            "453/453 [==============================] - 0s 308us/sample - loss: 2.2082 - accuracy: 0.6026\n",
            "Epoch 96/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 2.1907 - accuracy: 0.6093\n",
            "Epoch 97/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 2.1699 - accuracy: 0.6026\n",
            "Epoch 98/500\n",
            "453/453 [==============================] - 0s 280us/sample - loss: 2.1595 - accuracy: 0.6269\n",
            "Epoch 99/500\n",
            "453/453 [==============================] - 0s 296us/sample - loss: 2.1308 - accuracy: 0.6247\n",
            "Epoch 100/500\n",
            "453/453 [==============================] - 0s 327us/sample - loss: 2.1151 - accuracy: 0.6402\n",
            "Epoch 101/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 2.1064 - accuracy: 0.6336\n",
            "Epoch 102/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 2.0827 - accuracy: 0.6380\n",
            "Epoch 103/500\n",
            "453/453 [==============================] - 0s 280us/sample - loss: 2.0529 - accuracy: 0.6600\n",
            "Epoch 104/500\n",
            "453/453 [==============================] - 0s 335us/sample - loss: 2.0326 - accuracy: 0.6512\n",
            "Epoch 105/500\n",
            "453/453 [==============================] - 0s 305us/sample - loss: 2.0225 - accuracy: 0.6556\n",
            "Epoch 106/500\n",
            "453/453 [==============================] - 0s 295us/sample - loss: 2.0018 - accuracy: 0.6689\n",
            "Epoch 107/500\n",
            "453/453 [==============================] - 0s 331us/sample - loss: 1.9795 - accuracy: 0.6777\n",
            "Epoch 108/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 1.9626 - accuracy: 0.6821\n",
            "Epoch 109/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 1.9401 - accuracy: 0.6821\n",
            "Epoch 110/500\n",
            "453/453 [==============================] - 0s 301us/sample - loss: 1.9146 - accuracy: 0.6843\n",
            "Epoch 111/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 1.8991 - accuracy: 0.6887\n",
            "Epoch 112/500\n",
            "453/453 [==============================] - 0s 295us/sample - loss: 1.8878 - accuracy: 0.7042\n",
            "Epoch 113/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 1.8655 - accuracy: 0.6976\n",
            "Epoch 114/500\n",
            "453/453 [==============================] - 0s 303us/sample - loss: 1.8441 - accuracy: 0.7086\n",
            "Epoch 115/500\n",
            "453/453 [==============================] - 0s 316us/sample - loss: 1.8260 - accuracy: 0.7130\n",
            "Epoch 116/500\n",
            "453/453 [==============================] - 0s 282us/sample - loss: 1.8057 - accuracy: 0.7086\n",
            "Epoch 117/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 1.7881 - accuracy: 0.7219\n",
            "Epoch 118/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 1.7754 - accuracy: 0.7196\n",
            "Epoch 119/500\n",
            "453/453 [==============================] - 0s 312us/sample - loss: 1.7557 - accuracy: 0.7196\n",
            "Epoch 120/500\n",
            "453/453 [==============================] - 0s 273us/sample - loss: 1.7391 - accuracy: 0.7130\n",
            "Epoch 121/500\n",
            "453/453 [==============================] - 0s 268us/sample - loss: 1.7211 - accuracy: 0.7285\n",
            "Epoch 122/500\n",
            "453/453 [==============================] - 0s 313us/sample - loss: 1.7031 - accuracy: 0.7307\n",
            "Epoch 123/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 1.6889 - accuracy: 0.7351\n",
            "Epoch 124/500\n",
            "453/453 [==============================] - 0s 262us/sample - loss: 1.6699 - accuracy: 0.7461\n",
            "Epoch 125/500\n",
            "453/453 [==============================] - 0s 264us/sample - loss: 1.6558 - accuracy: 0.7506\n",
            "Epoch 126/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 1.6398 - accuracy: 0.7572\n",
            "Epoch 127/500\n",
            "453/453 [==============================] - 0s 305us/sample - loss: 1.6278 - accuracy: 0.7483\n",
            "Epoch 128/500\n",
            "453/453 [==============================] - 0s 322us/sample - loss: 1.6138 - accuracy: 0.7483\n",
            "Epoch 129/500\n",
            "453/453 [==============================] - 0s 351us/sample - loss: 1.6158 - accuracy: 0.7483\n",
            "Epoch 130/500\n",
            "453/453 [==============================] - 0s 379us/sample - loss: 1.6263 - accuracy: 0.7395\n",
            "Epoch 131/500\n",
            "453/453 [==============================] - 0s 331us/sample - loss: 1.5906 - accuracy: 0.7417\n",
            "Epoch 132/500\n",
            "453/453 [==============================] - 0s 332us/sample - loss: 1.5672 - accuracy: 0.7550\n",
            "Epoch 133/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 1.5523 - accuracy: 0.7572\n",
            "Epoch 134/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 1.5357 - accuracy: 0.7528\n",
            "Epoch 135/500\n",
            "453/453 [==============================] - 0s 270us/sample - loss: 1.5152 - accuracy: 0.7682\n",
            "Epoch 136/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 1.5003 - accuracy: 0.7748\n",
            "Epoch 137/500\n",
            "453/453 [==============================] - 0s 313us/sample - loss: 1.4871 - accuracy: 0.7770\n",
            "Epoch 138/500\n",
            "453/453 [==============================] - 0s 287us/sample - loss: 1.4729 - accuracy: 0.7748\n",
            "Epoch 139/500\n",
            "453/453 [==============================] - 0s 300us/sample - loss: 1.4569 - accuracy: 0.7792\n",
            "Epoch 140/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 1.4421 - accuracy: 0.7815\n",
            "Epoch 141/500\n",
            "453/453 [==============================] - 0s 303us/sample - loss: 1.4307 - accuracy: 0.7837\n",
            "Epoch 142/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 1.4154 - accuracy: 0.7859\n",
            "Epoch 143/500\n",
            "453/453 [==============================] - 0s 309us/sample - loss: 1.4137 - accuracy: 0.7925\n",
            "Epoch 144/500\n",
            "453/453 [==============================] - 0s 316us/sample - loss: 1.4177 - accuracy: 0.7859\n",
            "Epoch 145/500\n",
            "453/453 [==============================] - 0s 308us/sample - loss: 1.4061 - accuracy: 0.7881\n",
            "Epoch 146/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 1.3808 - accuracy: 0.8035\n",
            "Epoch 147/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 1.3623 - accuracy: 0.8057\n",
            "Epoch 148/500\n",
            "453/453 [==============================] - 0s 315us/sample - loss: 1.3538 - accuracy: 0.8035\n",
            "Epoch 149/500\n",
            "453/453 [==============================] - 0s 284us/sample - loss: 1.3427 - accuracy: 0.7991\n",
            "Epoch 150/500\n",
            "453/453 [==============================] - 0s 311us/sample - loss: 1.3262 - accuracy: 0.8057\n",
            "Epoch 151/500\n",
            "453/453 [==============================] - 0s 305us/sample - loss: 1.3170 - accuracy: 0.7991\n",
            "Epoch 152/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 1.2957 - accuracy: 0.8079\n",
            "Epoch 153/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 1.2837 - accuracy: 0.8057\n",
            "Epoch 154/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 1.2715 - accuracy: 0.8124\n",
            "Epoch 155/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 1.2670 - accuracy: 0.8212\n",
            "Epoch 156/500\n",
            "453/453 [==============================] - 0s 319us/sample - loss: 1.2495 - accuracy: 0.8212\n",
            "Epoch 157/500\n",
            "453/453 [==============================] - 0s 354us/sample - loss: 1.2337 - accuracy: 0.8190\n",
            "Epoch 158/500\n",
            "453/453 [==============================] - 0s 356us/sample - loss: 1.2206 - accuracy: 0.8344\n",
            "Epoch 159/500\n",
            "453/453 [==============================] - 0s 387us/sample - loss: 1.2098 - accuracy: 0.8300\n",
            "Epoch 160/500\n",
            "453/453 [==============================] - 0s 353us/sample - loss: 1.2077 - accuracy: 0.8300\n",
            "Epoch 161/500\n",
            "453/453 [==============================] - 0s 293us/sample - loss: 1.2030 - accuracy: 0.8212\n",
            "Epoch 162/500\n",
            "453/453 [==============================] - 0s 309us/sample - loss: 1.1861 - accuracy: 0.8344\n",
            "Epoch 163/500\n",
            "453/453 [==============================] - 0s 331us/sample - loss: 1.1614 - accuracy: 0.8344\n",
            "Epoch 164/500\n",
            "453/453 [==============================] - 0s 341us/sample - loss: 1.1538 - accuracy: 0.8300\n",
            "Epoch 165/500\n",
            "453/453 [==============================] - 0s 373us/sample - loss: 1.1398 - accuracy: 0.8411\n",
            "Epoch 166/500\n",
            "453/453 [==============================] - 0s 330us/sample - loss: 1.1285 - accuracy: 0.8389\n",
            "Epoch 167/500\n",
            "453/453 [==============================] - 0s 346us/sample - loss: 1.1186 - accuracy: 0.8411\n",
            "Epoch 168/500\n",
            "453/453 [==============================] - 0s 344us/sample - loss: 1.1069 - accuracy: 0.8433\n",
            "Epoch 169/500\n",
            "453/453 [==============================] - 0s 327us/sample - loss: 1.1024 - accuracy: 0.8433\n",
            "Epoch 170/500\n",
            "453/453 [==============================] - 0s 331us/sample - loss: 1.1139 - accuracy: 0.8433\n",
            "Epoch 171/500\n",
            "453/453 [==============================] - 0s 357us/sample - loss: 1.1069 - accuracy: 0.8433\n",
            "Epoch 172/500\n",
            "453/453 [==============================] - 0s 371us/sample - loss: 1.0938 - accuracy: 0.8499\n",
            "Epoch 173/500\n",
            "453/453 [==============================] - 0s 355us/sample - loss: 1.0896 - accuracy: 0.8477\n",
            "Epoch 174/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 1.0633 - accuracy: 0.8521\n",
            "Epoch 175/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 1.0544 - accuracy: 0.8455\n",
            "Epoch 176/500\n",
            "453/453 [==============================] - 0s 300us/sample - loss: 1.0450 - accuracy: 0.8477\n",
            "Epoch 177/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 1.0320 - accuracy: 0.8477\n",
            "Epoch 178/500\n",
            "453/453 [==============================] - 0s 284us/sample - loss: 1.0207 - accuracy: 0.8499\n",
            "Epoch 179/500\n",
            "453/453 [==============================] - 0s 327us/sample - loss: 1.0107 - accuracy: 0.8587\n",
            "Epoch 180/500\n",
            "453/453 [==============================] - 0s 283us/sample - loss: 1.0054 - accuracy: 0.8565\n",
            "Epoch 181/500\n",
            "453/453 [==============================] - 0s 328us/sample - loss: 1.0099 - accuracy: 0.8565\n",
            "Epoch 182/500\n",
            "453/453 [==============================] - 0s 304us/sample - loss: 1.0094 - accuracy: 0.8477\n",
            "Epoch 183/500\n",
            "453/453 [==============================] - 0s 297us/sample - loss: 1.0413 - accuracy: 0.8300\n",
            "Epoch 184/500\n",
            "453/453 [==============================] - 0s 268us/sample - loss: 1.0466 - accuracy: 0.8344\n",
            "Epoch 185/500\n",
            "453/453 [==============================] - 0s 274us/sample - loss: 1.0286 - accuracy: 0.8433\n",
            "Epoch 186/500\n",
            "453/453 [==============================] - 0s 332us/sample - loss: 1.0320 - accuracy: 0.8300\n",
            "Epoch 187/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 0.9924 - accuracy: 0.8455\n",
            "Epoch 188/500\n",
            "453/453 [==============================] - 0s 270us/sample - loss: 0.9634 - accuracy: 0.8609\n",
            "Epoch 189/500\n",
            "453/453 [==============================] - 0s 299us/sample - loss: 0.9557 - accuracy: 0.8653\n",
            "Epoch 190/500\n",
            "453/453 [==============================] - 0s 282us/sample - loss: 0.9257 - accuracy: 0.8675\n",
            "Epoch 191/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 0.9196 - accuracy: 0.8631\n",
            "Epoch 192/500\n",
            "453/453 [==============================] - 0s 322us/sample - loss: 0.9042 - accuracy: 0.8698\n",
            "Epoch 193/500\n",
            "453/453 [==============================] - 0s 321us/sample - loss: 0.8922 - accuracy: 0.8631\n",
            "Epoch 194/500\n",
            "453/453 [==============================] - 0s 320us/sample - loss: 0.8798 - accuracy: 0.8720\n",
            "Epoch 195/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 0.8693 - accuracy: 0.8720\n",
            "Epoch 196/500\n",
            "453/453 [==============================] - 0s 337us/sample - loss: 0.8656 - accuracy: 0.8742\n",
            "Epoch 197/500\n",
            "453/453 [==============================] - 0s 281us/sample - loss: 0.8601 - accuracy: 0.8786\n",
            "Epoch 198/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 0.8497 - accuracy: 0.8786\n",
            "Epoch 199/500\n",
            "453/453 [==============================] - 0s 284us/sample - loss: 0.8429 - accuracy: 0.8742\n",
            "Epoch 200/500\n",
            "453/453 [==============================] - 0s 312us/sample - loss: 0.8317 - accuracy: 0.8786\n",
            "Epoch 201/500\n",
            "453/453 [==============================] - 0s 316us/sample - loss: 0.8234 - accuracy: 0.8786\n",
            "Epoch 202/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 0.8144 - accuracy: 0.8786\n",
            "Epoch 203/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 0.8093 - accuracy: 0.8830\n",
            "Epoch 204/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 0.7979 - accuracy: 0.8830\n",
            "Epoch 205/500\n",
            "453/453 [==============================] - 0s 270us/sample - loss: 0.7899 - accuracy: 0.8874\n",
            "Epoch 206/500\n",
            "453/453 [==============================] - 0s 311us/sample - loss: 0.7836 - accuracy: 0.8962\n",
            "Epoch 207/500\n",
            "453/453 [==============================] - 0s 305us/sample - loss: 0.7768 - accuracy: 0.8940\n",
            "Epoch 208/500\n",
            "453/453 [==============================] - 0s 299us/sample - loss: 0.7702 - accuracy: 0.9029\n",
            "Epoch 209/500\n",
            "453/453 [==============================] - 0s 304us/sample - loss: 0.7625 - accuracy: 0.9051\n",
            "Epoch 210/500\n",
            "453/453 [==============================] - 0s 297us/sample - loss: 0.7554 - accuracy: 0.9007\n",
            "Epoch 211/500\n",
            "453/453 [==============================] - 0s 278us/sample - loss: 0.7494 - accuracy: 0.9029\n",
            "Epoch 212/500\n",
            "453/453 [==============================] - 0s 303us/sample - loss: 0.7429 - accuracy: 0.9029\n",
            "Epoch 213/500\n",
            "453/453 [==============================] - 0s 271us/sample - loss: 0.7354 - accuracy: 0.9073\n",
            "Epoch 214/500\n",
            "453/453 [==============================] - 0s 308us/sample - loss: 0.7289 - accuracy: 0.9073\n",
            "Epoch 215/500\n",
            "453/453 [==============================] - 0s 296us/sample - loss: 0.7265 - accuracy: 0.9051\n",
            "Epoch 216/500\n",
            "453/453 [==============================] - 0s 332us/sample - loss: 0.7217 - accuracy: 0.9073\n",
            "Epoch 217/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 0.7225 - accuracy: 0.9051\n",
            "Epoch 218/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 0.7274 - accuracy: 0.9029\n",
            "Epoch 219/500\n",
            "453/453 [==============================] - 0s 264us/sample - loss: 0.7236 - accuracy: 0.8962\n",
            "Epoch 220/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 0.7198 - accuracy: 0.8962\n",
            "Epoch 221/500\n",
            "453/453 [==============================] - 0s 266us/sample - loss: 0.7165 - accuracy: 0.9007\n",
            "Epoch 222/500\n",
            "453/453 [==============================] - 0s 305us/sample - loss: 0.6957 - accuracy: 0.9051\n",
            "Epoch 223/500\n",
            "453/453 [==============================] - 0s 354us/sample - loss: 0.6843 - accuracy: 0.9073\n",
            "Epoch 224/500\n",
            "453/453 [==============================] - 0s 305us/sample - loss: 0.6799 - accuracy: 0.9051\n",
            "Epoch 225/500\n",
            "453/453 [==============================] - 0s 281us/sample - loss: 0.6730 - accuracy: 0.9073\n",
            "Epoch 226/500\n",
            "453/453 [==============================] - 0s 268us/sample - loss: 0.6694 - accuracy: 0.9051\n",
            "Epoch 227/500\n",
            "453/453 [==============================] - 0s 274us/sample - loss: 0.6609 - accuracy: 0.9073\n",
            "Epoch 228/500\n",
            "453/453 [==============================] - 0s 301us/sample - loss: 0.6548 - accuracy: 0.9051\n",
            "Epoch 229/500\n",
            "453/453 [==============================] - 0s 301us/sample - loss: 0.6487 - accuracy: 0.9073\n",
            "Epoch 230/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 0.6417 - accuracy: 0.9073\n",
            "Epoch 231/500\n",
            "453/453 [==============================] - 0s 318us/sample - loss: 0.6363 - accuracy: 0.9073\n",
            "Epoch 232/500\n",
            "453/453 [==============================] - 0s 282us/sample - loss: 0.6334 - accuracy: 0.9117\n",
            "Epoch 233/500\n",
            "453/453 [==============================] - 0s 312us/sample - loss: 0.6273 - accuracy: 0.9139\n",
            "Epoch 234/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 0.6234 - accuracy: 0.9117\n",
            "Epoch 235/500\n",
            "453/453 [==============================] - 0s 313us/sample - loss: 0.6182 - accuracy: 0.9117\n",
            "Epoch 236/500\n",
            "453/453 [==============================] - 0s 283us/sample - loss: 0.6122 - accuracy: 0.9139\n",
            "Epoch 237/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 0.6068 - accuracy: 0.9161\n",
            "Epoch 238/500\n",
            "453/453 [==============================] - 0s 329us/sample - loss: 0.6014 - accuracy: 0.9161\n",
            "Epoch 239/500\n",
            "453/453 [==============================] - 0s 319us/sample - loss: 0.5973 - accuracy: 0.9139\n",
            "Epoch 240/500\n",
            "453/453 [==============================] - 0s 298us/sample - loss: 0.5934 - accuracy: 0.9205\n",
            "Epoch 241/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 0.5895 - accuracy: 0.9205\n",
            "Epoch 242/500\n",
            "453/453 [==============================] - 0s 268us/sample - loss: 0.5817 - accuracy: 0.9227\n",
            "Epoch 243/500\n",
            "453/453 [==============================] - 0s 284us/sample - loss: 0.5777 - accuracy: 0.9249\n",
            "Epoch 244/500\n",
            "453/453 [==============================] - 0s 293us/sample - loss: 0.5742 - accuracy: 0.9227\n",
            "Epoch 245/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 0.5722 - accuracy: 0.9227\n",
            "Epoch 246/500\n",
            "453/453 [==============================] - 0s 317us/sample - loss: 0.5645 - accuracy: 0.9272\n",
            "Epoch 247/500\n",
            "453/453 [==============================] - 0s 296us/sample - loss: 0.5660 - accuracy: 0.9227\n",
            "Epoch 248/500\n",
            "453/453 [==============================] - 0s 333us/sample - loss: 0.5615 - accuracy: 0.9249\n",
            "Epoch 249/500\n",
            "453/453 [==============================] - 0s 353us/sample - loss: 0.5577 - accuracy: 0.9227\n",
            "Epoch 250/500\n",
            "453/453 [==============================] - 0s 344us/sample - loss: 0.5496 - accuracy: 0.9294\n",
            "Epoch 251/500\n",
            "453/453 [==============================] - 0s 334us/sample - loss: 0.5432 - accuracy: 0.9294\n",
            "Epoch 252/500\n",
            "453/453 [==============================] - 0s 375us/sample - loss: 0.5369 - accuracy: 0.9294\n",
            "Epoch 253/500\n",
            "453/453 [==============================] - 0s 348us/sample - loss: 0.5330 - accuracy: 0.9272\n",
            "Epoch 254/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 0.5335 - accuracy: 0.9249\n",
            "Epoch 255/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 0.5269 - accuracy: 0.9272\n",
            "Epoch 256/500\n",
            "453/453 [==============================] - 0s 295us/sample - loss: 0.5245 - accuracy: 0.9227\n",
            "Epoch 257/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 0.5170 - accuracy: 0.9294\n",
            "Epoch 258/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 0.5116 - accuracy: 0.9272\n",
            "Epoch 259/500\n",
            "453/453 [==============================] - 0s 318us/sample - loss: 0.5083 - accuracy: 0.9294\n",
            "Epoch 260/500\n",
            "453/453 [==============================] - 0s 306us/sample - loss: 0.5089 - accuracy: 0.9249\n",
            "Epoch 261/500\n",
            "453/453 [==============================] - 0s 273us/sample - loss: 0.5294 - accuracy: 0.9205\n",
            "Epoch 262/500\n",
            "453/453 [==============================] - 0s 299us/sample - loss: 0.5246 - accuracy: 0.9205\n",
            "Epoch 263/500\n",
            "453/453 [==============================] - 0s 307us/sample - loss: 0.5219 - accuracy: 0.9183\n",
            "Epoch 264/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 0.5119 - accuracy: 0.9205\n",
            "Epoch 265/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 0.5068 - accuracy: 0.9183\n",
            "Epoch 266/500\n",
            "453/453 [==============================] - 0s 306us/sample - loss: 0.5119 - accuracy: 0.9161\n",
            "Epoch 267/500\n",
            "453/453 [==============================] - 0s 314us/sample - loss: 0.5067 - accuracy: 0.9227\n",
            "Epoch 268/500\n",
            "453/453 [==============================] - 0s 301us/sample - loss: 0.4900 - accuracy: 0.9249\n",
            "Epoch 269/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 0.4808 - accuracy: 0.9316\n",
            "Epoch 270/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 0.4771 - accuracy: 0.9249\n",
            "Epoch 271/500\n",
            "453/453 [==============================] - 0s 293us/sample - loss: 0.4727 - accuracy: 0.9338\n",
            "Epoch 272/500\n",
            "453/453 [==============================] - 0s 274us/sample - loss: 0.4654 - accuracy: 0.9294\n",
            "Epoch 273/500\n",
            "453/453 [==============================] - 0s 315us/sample - loss: 0.4622 - accuracy: 0.9272\n",
            "Epoch 274/500\n",
            "453/453 [==============================] - 0s 301us/sample - loss: 0.4572 - accuracy: 0.9338\n",
            "Epoch 275/500\n",
            "453/453 [==============================] - 0s 315us/sample - loss: 0.4516 - accuracy: 0.9338\n",
            "Epoch 276/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 0.4475 - accuracy: 0.9294\n",
            "Epoch 277/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 0.4423 - accuracy: 0.9360\n",
            "Epoch 278/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 0.4419 - accuracy: 0.9294\n",
            "Epoch 279/500\n",
            "453/453 [==============================] - 0s 268us/sample - loss: 0.4407 - accuracy: 0.9316\n",
            "Epoch 280/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 0.4361 - accuracy: 0.9294\n",
            "Epoch 281/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 0.4325 - accuracy: 0.9338\n",
            "Epoch 282/500\n",
            "453/453 [==============================] - 0s 303us/sample - loss: 0.4269 - accuracy: 0.9294\n",
            "Epoch 283/500\n",
            "453/453 [==============================] - 0s 315us/sample - loss: 0.4226 - accuracy: 0.9360\n",
            "Epoch 284/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 0.4201 - accuracy: 0.9360\n",
            "Epoch 285/500\n",
            "453/453 [==============================] - 0s 274us/sample - loss: 0.4193 - accuracy: 0.9360\n",
            "Epoch 286/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 0.4176 - accuracy: 0.9360\n",
            "Epoch 287/500\n",
            "453/453 [==============================] - 0s 280us/sample - loss: 0.4151 - accuracy: 0.9360\n",
            "Epoch 288/500\n",
            "453/453 [==============================] - 0s 306us/sample - loss: 0.4134 - accuracy: 0.9360\n",
            "Epoch 289/500\n",
            "453/453 [==============================] - 0s 332us/sample - loss: 0.4154 - accuracy: 0.9316\n",
            "Epoch 290/500\n",
            "453/453 [==============================] - 0s 348us/sample - loss: 0.4253 - accuracy: 0.9227\n",
            "Epoch 291/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 0.4127 - accuracy: 0.9360\n",
            "Epoch 292/500\n",
            "453/453 [==============================] - 0s 287us/sample - loss: 0.4130 - accuracy: 0.9360\n",
            "Epoch 293/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 0.4037 - accuracy: 0.9360\n",
            "Epoch 294/500\n",
            "453/453 [==============================] - 0s 275us/sample - loss: 0.3992 - accuracy: 0.9338\n",
            "Epoch 295/500\n",
            "453/453 [==============================] - 0s 282us/sample - loss: 0.3969 - accuracy: 0.9360\n",
            "Epoch 296/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 0.3908 - accuracy: 0.9382\n",
            "Epoch 297/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 0.3862 - accuracy: 0.9360\n",
            "Epoch 298/500\n",
            "453/453 [==============================] - 0s 337us/sample - loss: 0.3828 - accuracy: 0.9360\n",
            "Epoch 299/500\n",
            "453/453 [==============================] - 0s 300us/sample - loss: 0.3796 - accuracy: 0.9382\n",
            "Epoch 300/500\n",
            "453/453 [==============================] - 0s 297us/sample - loss: 0.3746 - accuracy: 0.9360\n",
            "Epoch 301/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 0.3734 - accuracy: 0.9338\n",
            "Epoch 302/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 0.3708 - accuracy: 0.9338\n",
            "Epoch 303/500\n",
            "453/453 [==============================] - 0s 278us/sample - loss: 0.3678 - accuracy: 0.9338\n",
            "Epoch 304/500\n",
            "453/453 [==============================] - 0s 301us/sample - loss: 0.3651 - accuracy: 0.9382\n",
            "Epoch 305/500\n",
            "453/453 [==============================] - 0s 300us/sample - loss: 0.3620 - accuracy: 0.9382\n",
            "Epoch 306/500\n",
            "453/453 [==============================] - 0s 271us/sample - loss: 0.3648 - accuracy: 0.9360\n",
            "Epoch 307/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 0.3647 - accuracy: 0.9382\n",
            "Epoch 308/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 0.3588 - accuracy: 0.9338\n",
            "Epoch 309/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 0.3550 - accuracy: 0.9338\n",
            "Epoch 310/500\n",
            "453/453 [==============================] - 0s 281us/sample - loss: 0.3542 - accuracy: 0.9316\n",
            "Epoch 311/500\n",
            "453/453 [==============================] - 0s 277us/sample - loss: 0.3546 - accuracy: 0.9360\n",
            "Epoch 312/500\n",
            "453/453 [==============================] - 0s 312us/sample - loss: 0.3495 - accuracy: 0.9360\n",
            "Epoch 313/500\n",
            "453/453 [==============================] - 0s 312us/sample - loss: 0.3463 - accuracy: 0.9404\n",
            "Epoch 314/500\n",
            "453/453 [==============================] - 0s 279us/sample - loss: 0.3434 - accuracy: 0.9404\n",
            "Epoch 315/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 0.3412 - accuracy: 0.9382\n",
            "Epoch 316/500\n",
            "453/453 [==============================] - 0s 276us/sample - loss: 0.3381 - accuracy: 0.9426\n",
            "Epoch 317/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 0.3360 - accuracy: 0.9404\n",
            "Epoch 318/500\n",
            "453/453 [==============================] - 0s 293us/sample - loss: 0.3342 - accuracy: 0.9404\n",
            "Epoch 319/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 0.3321 - accuracy: 0.9382\n",
            "Epoch 320/500\n",
            "453/453 [==============================] - 0s 298us/sample - loss: 0.3298 - accuracy: 0.9360\n",
            "Epoch 321/500\n",
            "453/453 [==============================] - 0s 338us/sample - loss: 0.3284 - accuracy: 0.9404\n",
            "Epoch 322/500\n",
            "453/453 [==============================] - 0s 344us/sample - loss: 0.3256 - accuracy: 0.9426\n",
            "Epoch 323/500\n",
            "453/453 [==============================] - 0s 331us/sample - loss: 0.3246 - accuracy: 0.9426\n",
            "Epoch 324/500\n",
            "453/453 [==============================] - 0s 349us/sample - loss: 0.3249 - accuracy: 0.9382\n",
            "Epoch 325/500\n",
            "453/453 [==============================] - 0s 333us/sample - loss: 0.3221 - accuracy: 0.9404\n",
            "Epoch 326/500\n",
            "453/453 [==============================] - 0s 329us/sample - loss: 0.3204 - accuracy: 0.9382\n",
            "Epoch 327/500\n",
            "453/453 [==============================] - 0s 303us/sample - loss: 0.3201 - accuracy: 0.9426\n",
            "Epoch 328/500\n",
            "453/453 [==============================] - 0s 307us/sample - loss: 0.3201 - accuracy: 0.9360\n",
            "Epoch 329/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 0.3262 - accuracy: 0.9360\n",
            "Epoch 330/500\n",
            "453/453 [==============================] - 0s 300us/sample - loss: 0.3459 - accuracy: 0.9316\n",
            "Epoch 331/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 0.3663 - accuracy: 0.9227\n",
            "Epoch 332/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 0.4209 - accuracy: 0.9029\n",
            "Epoch 333/500\n",
            "453/453 [==============================] - 0s 296us/sample - loss: 0.4330 - accuracy: 0.9029\n",
            "Epoch 334/500\n",
            "453/453 [==============================] - 0s 280us/sample - loss: 0.4022 - accuracy: 0.9095\n",
            "Epoch 335/500\n",
            "453/453 [==============================] - 0s 307us/sample - loss: 0.3690 - accuracy: 0.9205\n",
            "Epoch 336/500\n",
            "453/453 [==============================] - 0s 277us/sample - loss: 0.3458 - accuracy: 0.9338\n",
            "Epoch 337/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 0.3287 - accuracy: 0.9404\n",
            "Epoch 338/500\n",
            "453/453 [==============================] - 0s 289us/sample - loss: 0.3191 - accuracy: 0.9360\n",
            "Epoch 339/500\n",
            "453/453 [==============================] - 0s 276us/sample - loss: 0.3186 - accuracy: 0.9382\n",
            "Epoch 340/500\n",
            "453/453 [==============================] - 0s 293us/sample - loss: 0.3110 - accuracy: 0.9404\n",
            "Epoch 341/500\n",
            "453/453 [==============================] - 0s 309us/sample - loss: 0.3049 - accuracy: 0.9382\n",
            "Epoch 342/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 0.3016 - accuracy: 0.9360\n",
            "Epoch 343/500\n",
            "453/453 [==============================] - 0s 304us/sample - loss: 0.2991 - accuracy: 0.9426\n",
            "Epoch 344/500\n",
            "453/453 [==============================] - 0s 267us/sample - loss: 0.2946 - accuracy: 0.9382\n",
            "Epoch 345/500\n",
            "453/453 [==============================] - 0s 299us/sample - loss: 0.2925 - accuracy: 0.9382\n",
            "Epoch 346/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 0.2892 - accuracy: 0.9426\n",
            "Epoch 347/500\n",
            "453/453 [==============================] - 0s 269us/sample - loss: 0.2880 - accuracy: 0.9404\n",
            "Epoch 348/500\n",
            "453/453 [==============================] - 0s 295us/sample - loss: 0.2910 - accuracy: 0.9404\n",
            "Epoch 349/500\n",
            "453/453 [==============================] - 0s 275us/sample - loss: 0.2857 - accuracy: 0.9404\n",
            "Epoch 350/500\n",
            "453/453 [==============================] - 0s 321us/sample - loss: 0.2856 - accuracy: 0.9404\n",
            "Epoch 351/500\n",
            "453/453 [==============================] - 0s 262us/sample - loss: 0.2799 - accuracy: 0.9382\n",
            "Epoch 352/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 0.2790 - accuracy: 0.9382\n",
            "Epoch 353/500\n",
            "453/453 [==============================] - 0s 268us/sample - loss: 0.2776 - accuracy: 0.9382\n",
            "Epoch 354/500\n",
            "453/453 [==============================] - 0s 283us/sample - loss: 0.2750 - accuracy: 0.9382\n",
            "Epoch 355/500\n",
            "453/453 [==============================] - 0s 264us/sample - loss: 0.2726 - accuracy: 0.9404\n",
            "Epoch 356/500\n",
            "453/453 [==============================] - 0s 308us/sample - loss: 0.2723 - accuracy: 0.9382\n",
            "Epoch 357/500\n",
            "453/453 [==============================] - 0s 266us/sample - loss: 0.2709 - accuracy: 0.9426\n",
            "Epoch 358/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 0.2676 - accuracy: 0.9426\n",
            "Epoch 359/500\n",
            "453/453 [==============================] - 0s 287us/sample - loss: 0.2671 - accuracy: 0.9470\n",
            "Epoch 360/500\n",
            "453/453 [==============================] - 0s 280us/sample - loss: 0.2637 - accuracy: 0.9448\n",
            "Epoch 361/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 0.2622 - accuracy: 0.9426\n",
            "Epoch 362/500\n",
            "453/453 [==============================] - 0s 263us/sample - loss: 0.2601 - accuracy: 0.9470\n",
            "Epoch 363/500\n",
            "453/453 [==============================] - 0s 287us/sample - loss: 0.2586 - accuracy: 0.9470\n",
            "Epoch 364/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 0.2586 - accuracy: 0.9492\n",
            "Epoch 365/500\n",
            "453/453 [==============================] - 0s 296us/sample - loss: 0.2579 - accuracy: 0.9492\n",
            "Epoch 366/500\n",
            "453/453 [==============================] - 0s 305us/sample - loss: 0.2564 - accuracy: 0.9470\n",
            "Epoch 367/500\n",
            "453/453 [==============================] - 0s 268us/sample - loss: 0.2710 - accuracy: 0.9426\n",
            "Epoch 368/500\n",
            "453/453 [==============================] - 0s 273us/sample - loss: 0.2683 - accuracy: 0.9404\n",
            "Epoch 369/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 0.2659 - accuracy: 0.9382\n",
            "Epoch 370/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 0.2601 - accuracy: 0.9448\n",
            "Epoch 371/500\n",
            "453/453 [==============================] - 0s 297us/sample - loss: 0.2562 - accuracy: 0.9470\n",
            "Epoch 372/500\n",
            "453/453 [==============================] - 0s 307us/sample - loss: 0.2507 - accuracy: 0.9448\n",
            "Epoch 373/500\n",
            "453/453 [==============================] - 0s 293us/sample - loss: 0.2464 - accuracy: 0.9470\n",
            "Epoch 374/500\n",
            "453/453 [==============================] - 0s 335us/sample - loss: 0.2441 - accuracy: 0.9470\n",
            "Epoch 375/500\n",
            "453/453 [==============================] - 0s 264us/sample - loss: 0.2416 - accuracy: 0.9492\n",
            "Epoch 376/500\n",
            "453/453 [==============================] - 0s 256us/sample - loss: 0.2417 - accuracy: 0.9492\n",
            "Epoch 377/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.2397 - accuracy: 0.9470\n",
            "Epoch 378/500\n",
            "453/453 [==============================] - 0s 270us/sample - loss: 0.2392 - accuracy: 0.9470\n",
            "Epoch 379/500\n",
            "453/453 [==============================] - 0s 305us/sample - loss: 0.2369 - accuracy: 0.9492\n",
            "Epoch 380/500\n",
            "453/453 [==============================] - 0s 262us/sample - loss: 0.2354 - accuracy: 0.9492\n",
            "Epoch 381/500\n",
            "453/453 [==============================] - 0s 270us/sample - loss: 0.2333 - accuracy: 0.9492\n",
            "Epoch 382/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 0.2326 - accuracy: 0.9470\n",
            "Epoch 383/500\n",
            "453/453 [==============================] - 0s 266us/sample - loss: 0.2320 - accuracy: 0.9492\n",
            "Epoch 384/500\n",
            "453/453 [==============================] - 0s 269us/sample - loss: 0.2308 - accuracy: 0.9448\n",
            "Epoch 385/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 0.2296 - accuracy: 0.9448\n",
            "Epoch 386/500\n",
            "453/453 [==============================] - 0s 261us/sample - loss: 0.2283 - accuracy: 0.9448\n",
            "Epoch 387/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 0.2272 - accuracy: 0.9448\n",
            "Epoch 388/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 0.2254 - accuracy: 0.9470\n",
            "Epoch 389/500\n",
            "453/453 [==============================] - 0s 378us/sample - loss: 0.2248 - accuracy: 0.9426\n",
            "Epoch 390/500\n",
            "453/453 [==============================] - 0s 330us/sample - loss: 0.2237 - accuracy: 0.9448\n",
            "Epoch 391/500\n",
            "453/453 [==============================] - 0s 333us/sample - loss: 0.2226 - accuracy: 0.9426\n",
            "Epoch 392/500\n",
            "453/453 [==============================] - 0s 348us/sample - loss: 0.2215 - accuracy: 0.9448\n",
            "Epoch 393/500\n",
            "453/453 [==============================] - 0s 359us/sample - loss: 0.2200 - accuracy: 0.9492\n",
            "Epoch 394/500\n",
            "453/453 [==============================] - 0s 307us/sample - loss: 0.2194 - accuracy: 0.9470\n",
            "Epoch 395/500\n",
            "453/453 [==============================] - 0s 347us/sample - loss: 0.2182 - accuracy: 0.9492\n",
            "Epoch 396/500\n",
            "453/453 [==============================] - 0s 359us/sample - loss: 0.2180 - accuracy: 0.9470\n",
            "Epoch 397/500\n",
            "453/453 [==============================] - 0s 337us/sample - loss: 0.2165 - accuracy: 0.9492\n",
            "Epoch 398/500\n",
            "453/453 [==============================] - 0s 335us/sample - loss: 0.2152 - accuracy: 0.9470\n",
            "Epoch 399/500\n",
            "453/453 [==============================] - 0s 340us/sample - loss: 0.2154 - accuracy: 0.9448\n",
            "Epoch 400/500\n",
            "453/453 [==============================] - 0s 352us/sample - loss: 0.2141 - accuracy: 0.9470\n",
            "Epoch 401/500\n",
            "453/453 [==============================] - 0s 346us/sample - loss: 0.2130 - accuracy: 0.9448\n",
            "Epoch 402/500\n",
            "453/453 [==============================] - 0s 366us/sample - loss: 0.2110 - accuracy: 0.9492\n",
            "Epoch 403/500\n",
            "453/453 [==============================] - 0s 339us/sample - loss: 0.2104 - accuracy: 0.9492\n",
            "Epoch 404/500\n",
            "453/453 [==============================] - 0s 346us/sample - loss: 0.2090 - accuracy: 0.9492\n",
            "Epoch 405/500\n",
            "453/453 [==============================] - 0s 343us/sample - loss: 0.2076 - accuracy: 0.9448\n",
            "Epoch 406/500\n",
            "453/453 [==============================] - 0s 345us/sample - loss: 0.2070 - accuracy: 0.9470\n",
            "Epoch 407/500\n",
            "453/453 [==============================] - 0s 317us/sample - loss: 0.2060 - accuracy: 0.9470\n",
            "Epoch 408/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 0.2056 - accuracy: 0.9492\n",
            "Epoch 409/500\n",
            "453/453 [==============================] - 0s 300us/sample - loss: 0.2043 - accuracy: 0.9448\n",
            "Epoch 410/500\n",
            "453/453 [==============================] - 0s 304us/sample - loss: 0.2061 - accuracy: 0.9426\n",
            "Epoch 411/500\n",
            "453/453 [==============================] - 0s 307us/sample - loss: 0.2030 - accuracy: 0.9514\n",
            "Epoch 412/500\n",
            "453/453 [==============================] - 0s 284us/sample - loss: 0.2016 - accuracy: 0.9470\n",
            "Epoch 413/500\n",
            "453/453 [==============================] - 0s 309us/sample - loss: 0.2011 - accuracy: 0.9514\n",
            "Epoch 414/500\n",
            "453/453 [==============================] - 0s 327us/sample - loss: 0.2007 - accuracy: 0.9448\n",
            "Epoch 415/500\n",
            "453/453 [==============================] - 0s 281us/sample - loss: 0.2006 - accuracy: 0.9492\n",
            "Epoch 416/500\n",
            "453/453 [==============================] - 0s 309us/sample - loss: 0.2019 - accuracy: 0.9426\n",
            "Epoch 417/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 0.1992 - accuracy: 0.9470\n",
            "Epoch 418/500\n",
            "453/453 [==============================] - 0s 295us/sample - loss: 0.1980 - accuracy: 0.9426\n",
            "Epoch 419/500\n",
            "453/453 [==============================] - 0s 325us/sample - loss: 0.1970 - accuracy: 0.9470\n",
            "Epoch 420/500\n",
            "453/453 [==============================] - 0s 353us/sample - loss: 0.1951 - accuracy: 0.9492\n",
            "Epoch 421/500\n",
            "453/453 [==============================] - 0s 295us/sample - loss: 0.1947 - accuracy: 0.9470\n",
            "Epoch 422/500\n",
            "453/453 [==============================] - 0s 263us/sample - loss: 0.1933 - accuracy: 0.9448\n",
            "Epoch 423/500\n",
            "453/453 [==============================] - 0s 301us/sample - loss: 0.1924 - accuracy: 0.9470\n",
            "Epoch 424/500\n",
            "453/453 [==============================] - 0s 303us/sample - loss: 0.1923 - accuracy: 0.9514\n",
            "Epoch 425/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 0.1912 - accuracy: 0.9492\n",
            "Epoch 426/500\n",
            "453/453 [==============================] - 0s 255us/sample - loss: 0.1909 - accuracy: 0.9404\n",
            "Epoch 427/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 0.1902 - accuracy: 0.9492\n",
            "Epoch 428/500\n",
            "453/453 [==============================] - 0s 284us/sample - loss: 0.1886 - accuracy: 0.9470\n",
            "Epoch 429/500\n",
            "453/453 [==============================] - 0s 297us/sample - loss: 0.1908 - accuracy: 0.9448\n",
            "Epoch 430/500\n",
            "453/453 [==============================] - 0s 274us/sample - loss: 0.1928 - accuracy: 0.9404\n",
            "Epoch 431/500\n",
            "453/453 [==============================] - 0s 316us/sample - loss: 0.1920 - accuracy: 0.9426\n",
            "Epoch 432/500\n",
            "453/453 [==============================] - 0s 322us/sample - loss: 0.1905 - accuracy: 0.9470\n",
            "Epoch 433/500\n",
            "453/453 [==============================] - 0s 317us/sample - loss: 0.1895 - accuracy: 0.9448\n",
            "Epoch 434/500\n",
            "453/453 [==============================] - 0s 313us/sample - loss: 0.1878 - accuracy: 0.9470\n",
            "Epoch 435/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 0.1859 - accuracy: 0.9470\n",
            "Epoch 436/500\n",
            "453/453 [==============================] - 0s 330us/sample - loss: 0.1844 - accuracy: 0.9470\n",
            "Epoch 437/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 0.1837 - accuracy: 0.9492\n",
            "Epoch 438/500\n",
            "453/453 [==============================] - 0s 269us/sample - loss: 0.1832 - accuracy: 0.9448\n",
            "Epoch 439/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 0.1832 - accuracy: 0.9492\n",
            "Epoch 440/500\n",
            "453/453 [==============================] - 0s 269us/sample - loss: 0.1868 - accuracy: 0.9492\n",
            "Epoch 441/500\n",
            "453/453 [==============================] - 0s 287us/sample - loss: 0.2040 - accuracy: 0.9404\n",
            "Epoch 442/500\n",
            "453/453 [==============================] - 0s 315us/sample - loss: 0.2410 - accuracy: 0.9360\n",
            "Epoch 443/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 0.2296 - accuracy: 0.9470\n",
            "Epoch 444/500\n",
            "453/453 [==============================] - 0s 282us/sample - loss: 0.2539 - accuracy: 0.9316\n",
            "Epoch 445/500\n",
            "453/453 [==============================] - 0s 300us/sample - loss: 0.2665 - accuracy: 0.9316\n",
            "Epoch 446/500\n",
            "453/453 [==============================] - 0s 320us/sample - loss: 0.2772 - accuracy: 0.9360\n",
            "Epoch 447/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 0.2294 - accuracy: 0.9382\n",
            "Epoch 448/500\n",
            "453/453 [==============================] - 0s 269us/sample - loss: 0.2090 - accuracy: 0.9426\n",
            "Epoch 449/500\n",
            "453/453 [==============================] - 0s 265us/sample - loss: 0.2106 - accuracy: 0.9360\n",
            "Epoch 450/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 0.1990 - accuracy: 0.9470\n",
            "Epoch 451/500\n",
            "453/453 [==============================] - 0s 295us/sample - loss: 0.1931 - accuracy: 0.9514\n",
            "Epoch 452/500\n",
            "453/453 [==============================] - 0s 281us/sample - loss: 0.1934 - accuracy: 0.9470\n",
            "Epoch 453/500\n",
            "453/453 [==============================] - 0s 269us/sample - loss: 0.1888 - accuracy: 0.9470\n",
            "Epoch 454/500\n",
            "453/453 [==============================] - 0s 320us/sample - loss: 0.1894 - accuracy: 0.9448\n",
            "Epoch 455/500\n",
            "453/453 [==============================] - 0s 284us/sample - loss: 0.1833 - accuracy: 0.9492\n",
            "Epoch 456/500\n",
            "453/453 [==============================] - 0s 269us/sample - loss: 0.1842 - accuracy: 0.9448\n",
            "Epoch 457/500\n",
            "453/453 [==============================] - 0s 315us/sample - loss: 0.1807 - accuracy: 0.9492\n",
            "Epoch 458/500\n",
            "453/453 [==============================] - 0s 297us/sample - loss: 0.1779 - accuracy: 0.9470\n",
            "Epoch 459/500\n",
            "453/453 [==============================] - 0s 280us/sample - loss: 0.1766 - accuracy: 0.9536\n",
            "Epoch 460/500\n",
            "453/453 [==============================] - 0s 279us/sample - loss: 0.1750 - accuracy: 0.9514\n",
            "Epoch 461/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 0.1741 - accuracy: 0.9514\n",
            "Epoch 462/500\n",
            "453/453 [==============================] - 0s 316us/sample - loss: 0.1729 - accuracy: 0.9470\n",
            "Epoch 463/500\n",
            "453/453 [==============================] - 0s 268us/sample - loss: 0.1711 - accuracy: 0.9492\n",
            "Epoch 464/500\n",
            "453/453 [==============================] - 0s 301us/sample - loss: 0.1710 - accuracy: 0.9470\n",
            "Epoch 465/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 0.1708 - accuracy: 0.9448\n",
            "Epoch 466/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 0.1707 - accuracy: 0.9492\n",
            "Epoch 467/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 0.1704 - accuracy: 0.9448\n",
            "Epoch 468/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 0.1687 - accuracy: 0.9492\n",
            "Epoch 469/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 0.1683 - accuracy: 0.9470\n",
            "Epoch 470/500\n",
            "453/453 [==============================] - 0s 291us/sample - loss: 0.1671 - accuracy: 0.9492\n",
            "Epoch 471/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 0.1658 - accuracy: 0.9514\n",
            "Epoch 472/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 0.1695 - accuracy: 0.9448\n",
            "Epoch 473/500\n",
            "453/453 [==============================] - 0s 300us/sample - loss: 0.1672 - accuracy: 0.9492\n",
            "Epoch 474/500\n",
            "453/453 [==============================] - 0s 316us/sample - loss: 0.1651 - accuracy: 0.9492\n",
            "Epoch 475/500\n",
            "453/453 [==============================] - 0s 295us/sample - loss: 0.1634 - accuracy: 0.9492\n",
            "Epoch 476/500\n",
            "453/453 [==============================] - 0s 293us/sample - loss: 0.1643 - accuracy: 0.9492\n",
            "Epoch 477/500\n",
            "453/453 [==============================] - 0s 317us/sample - loss: 0.1632 - accuracy: 0.9492\n",
            "Epoch 478/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 0.1616 - accuracy: 0.9492\n",
            "Epoch 479/500\n",
            "453/453 [==============================] - 0s 313us/sample - loss: 0.1610 - accuracy: 0.9514\n",
            "Epoch 480/500\n",
            "453/453 [==============================] - 0s 263us/sample - loss: 0.1601 - accuracy: 0.9514\n",
            "Epoch 481/500\n",
            "453/453 [==============================] - 0s 348us/sample - loss: 0.1600 - accuracy: 0.9536\n",
            "Epoch 482/500\n",
            "453/453 [==============================] - 0s 335us/sample - loss: 0.1589 - accuracy: 0.9536\n",
            "Epoch 483/500\n",
            "453/453 [==============================] - 0s 355us/sample - loss: 0.1591 - accuracy: 0.9492\n",
            "Epoch 484/500\n",
            "453/453 [==============================] - 0s 364us/sample - loss: 0.1581 - accuracy: 0.9426\n",
            "Epoch 485/500\n",
            "453/453 [==============================] - 0s 326us/sample - loss: 0.1576 - accuracy: 0.9492\n",
            "Epoch 486/500\n",
            "453/453 [==============================] - 0s 304us/sample - loss: 0.1578 - accuracy: 0.9492\n",
            "Epoch 487/500\n",
            "453/453 [==============================] - 0s 286us/sample - loss: 0.1571 - accuracy: 0.9382\n",
            "Epoch 488/500\n",
            "453/453 [==============================] - 0s 307us/sample - loss: 0.1571 - accuracy: 0.9448\n",
            "Epoch 489/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 0.1563 - accuracy: 0.9470\n",
            "Epoch 490/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 0.1552 - accuracy: 0.9492\n",
            "Epoch 491/500\n",
            "453/453 [==============================] - 0s 310us/sample - loss: 0.1553 - accuracy: 0.9470\n",
            "Epoch 492/500\n",
            "453/453 [==============================] - 0s 282us/sample - loss: 0.1556 - accuracy: 0.9470\n",
            "Epoch 493/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 0.1544 - accuracy: 0.9470\n",
            "Epoch 494/500\n",
            "453/453 [==============================] - 0s 275us/sample - loss: 0.1542 - accuracy: 0.9470\n",
            "Epoch 495/500\n",
            "453/453 [==============================] - 0s 339us/sample - loss: 0.1537 - accuracy: 0.9492\n",
            "Epoch 496/500\n",
            "453/453 [==============================] - 0s 356us/sample - loss: 0.1527 - accuracy: 0.9536\n",
            "Epoch 497/500\n",
            "453/453 [==============================] - 0s 346us/sample - loss: 0.1531 - accuracy: 0.9514\n",
            "Epoch 498/500\n",
            "453/453 [==============================] - 0s 381us/sample - loss: 0.1526 - accuracy: 0.9492\n",
            "Epoch 499/500\n",
            "453/453 [==============================] - 0s 335us/sample - loss: 0.1519 - accuracy: 0.9492\n",
            "Epoch 500/500\n",
            "453/453 [==============================] - 0s 336us/sample - loss: 0.1510 - accuracy: 0.9470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YXGelKThoTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poeprYK8h-c7",
        "colab_type": "code",
        "outputId": "91353745-f4fd-4588-864a-88e73fef22cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plot_graphs(history, 'accuracy')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2OSR6apvKue",
        "colab_type": "text"
      },
      "source": [
        "**Single prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5x0IHO5vI5Y",
        "colab_type": "code",
        "outputId": "d627bcfd-95a7-4bf5-e710-9c4e16cffe1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "seed_text = \"Laurence went to dublin\"\n",
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # max_sequence_len-1 , because initially we had last column as label, here no label\n",
        "\n",
        "xx = model.predict(token_list)\n",
        "print(xx.shape) # [[7.54368727e-07 3.03809959e-02 8.97304993e-03 6.51388466e-02.......]]\n",
        "\n",
        "predicted = model.predict_classes(token_list, verbose=0)\n",
        "print(predicted)\n",
        "\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  if index == predicted:\n",
        "    output_word = word\n",
        "    print(output_word)\n",
        "    break\n",
        "\n",
        "print(tokenizer.word_index['got'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 263)\n",
            "[23]\n",
            "got\n",
            "23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vc6PHgxa6Hm",
        "colab_type": "code",
        "outputId": "82c13fcb-a796-4b43-c271-8e1788f7fb75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "seed_text = \"Laurence went to dublin\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Laurence went to dublin got a through miss finnertys cask eyes me didnt forget me didnt creature a make mchugh arrived saw glisten eyes them fainted and might might would fall nonsense fall fall fall old fall eyes me didnt forget forget me didnt forget new catchers saw your saw glisten glisten eyes me creature long long out were all dublin runctions by dublin dublin lanigans ball hearty satisfaction the mad me entangled forget forget me saw didnt saw the ground fair casey the wall call taras call ask hall ask hearty me saw forget saw forget satisfaction swore saw her father fair a\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}